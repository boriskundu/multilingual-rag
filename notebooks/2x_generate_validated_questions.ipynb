{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1c7604e-9933-4fb7-b3ec-aba5f7441376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATING IDENTICAL QUESTIONS FOR HINDI AND CHINESE\n",
      "Strategy: Generate English questions, then translate to both languages\n",
      "================================================================================\n",
      "\n",
      "Total documents: 16036\n",
      "\n",
      "Documents by source:\n",
      "  FDA: 15942 documents\n",
      "  MedlinePlus (NIH): 94 documents\n",
      "\n",
      "================================================================================\n",
      "CORPUS ANALYSIS FOR QUESTION GENERATION\n",
      "================================================================================\n",
      "\n",
      "FDA - Sample content:\n",
      "INDICATIONS AND USAGE: 1 INDICATIONS AND USAGE Naproxen tablets and naproxen sodium tablets are indicated for: the relief of the signs and symptoms of: â€¢ rheumatoid arthritis â€¢ osteoarthritis â€¢ ankylosing spondylitis â€¢ Polyarticular Juvenile Idiopathic Arthritis Naproxen tablets and naproxen sodium tablets are also indicated for: the relief of signs and symptoms of: â€¢ tendonitis â€¢ bursitis â€¢ acute gout the management of: â€¢ pain â€¢ primary dysmenorrhea Naproxen tablets and naproxen sodium tablets ...\n",
      "\n",
      "MedlinePlus (NIH) - Sample content:\n",
      "SUMMARY: What is asthma?Asthma is a chronic (long-term) lung disease. It affects your airways, the tubes that carry air in and out of your lungs. When you have asthma, your airways can become inflamed and narrowed. This can cause wheezing,coughing, and tightness in your chest. When these symptoms get worse than usual, it is called an asthma attack or flare-up.What causes asthma?The exact cause of asthma is unknown. Genetics and your environment likely play a role in who gets asthma.An asthma att...\n",
      "\n",
      "================================================================================\n",
      "STEP 1: GENERATING BASE ENGLISH QUESTIONS\n",
      "================================================================================\n",
      "Generated 30 base English questions\n",
      "\n",
      "Sample English questions:\n",
      "  1. What is insulin resistance and how does it affect glucose entry into cells?\n",
      "     Category: condition | Complexity: moderate\n",
      "  2. How does family history influence the risk of heart disease?\n",
      "     Category: condition | Complexity: moderate\n",
      "  3. What are the drinking guidelines to reduce heart disease risk for men and women?\n",
      "     Category: treatment | Complexity: simple\n",
      "  4. What impact does smoking have on blood pressure and heart disease risk?\n",
      "     Category: condition | Complexity: moderate\n",
      "  5. What is mepivacaine hydrochloride used for?\n",
      "     Category: medication | Complexity: simple\n",
      "\n",
      "================================================================================\n",
      "STEP 2: VALIDATING QUESTIONS AGAINST CORPUS\n",
      "================================================================================\n",
      "Answerable questions: 30/30\n",
      "Final question set: 30 questions\n",
      "\n",
      "================================================================================\n",
      "STEP 3: TRANSLATING TO HINDI AND CHINESE\n",
      "================================================================================\n",
      "\n",
      "Translating questions to Hindi...\n",
      "   1. à¤‡à¤‚à¤¸à¥à¤²à¤¿à¤¨ à¤ªà¥à¤°à¤¤à¤¿à¤°à¥‹à¤§ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ à¤”à¤° à¤¯à¤¹ à¤•à¥‹à¤¶à¤¿à¤•à¤¾à¤“à¤‚ à¤®à¥‡à¤‚ à¤—à¥à¤²à¥‚à¤•à¥‹à¤œ à¤•à¥‡ à¤ªà¥à¤°à¤µà¥‡à¤¶ à¤•à¥‹ à¤•à¥ˆà¤¸à¥‡ à¤ªà¥à¤°à¤­à¤¾à¤µà¤¿à¤¤ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ?\n",
      "   2. à¤¹à¥ƒà¤¦à¤¯ à¤°à¥‹à¤— à¤•à¥‡ à¤œà¥‹à¤–à¤¿à¤® à¤ªà¤° à¤ªà¤¾à¤°à¤¿à¤µà¤¾à¤°à¤¿à¤• à¤‡à¤¤à¤¿à¤¹à¤¾à¤¸ à¤•à¤¾ à¤•à¥à¤¯à¤¾ à¤ªà¥à¤°à¤­à¤¾à¤µ à¤ªà¤¡à¤¼à¤¤à¤¾ à¤¹à¥ˆ?\n",
      "   3. à¤ªà¥à¤°à¥à¤·à¥‹à¤‚ à¤”à¤° à¤®à¤¹à¤¿à¤²à¤¾à¤“à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤¹à¥ƒà¤¦à¤¯ à¤°à¥‹à¤— à¤•à¥‡ à¤œà¥‹à¤–à¤¿à¤® à¤•à¥‹ à¤•à¤® à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤ªà¥€à¤¨à¥‡ à¤•à¥‡ à¤¦à¤¿à¤¶à¤¾-à¤¨à¤¿à¤°à¥à¤¦à¥‡à¤¶ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆà¤‚?\n",
      "   4. à¤§à¥‚à¤®à¥à¤°à¤ªà¤¾à¤¨ à¤•à¤¾ à¤°à¤•à¥à¤¤à¤šà¤¾à¤ª à¤”à¤° à¤¹à¥ƒà¤¦à¤¯ à¤°à¥‹à¤— à¤•à¥‡ à¤œà¥‹à¤–à¤¿à¤® à¤ªà¤° à¤•à¥à¤¯à¤¾ à¤ªà¥à¤°à¤­à¤¾à¤µ à¤ªà¤¡à¤¼à¤¤à¤¾ à¤¹à¥ˆ?\n",
      "   5. à¤®à¥‡à¤ªà¤¿à¤µà¤¾à¤•à¥‡à¤¨ à¤¹à¤¾à¤‡à¤¡à¥à¤°à¥‹à¤•à¥à¤²à¥‹à¤°à¤¾à¤‡à¤¡ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤¿à¤¸ à¤²à¤¿à¤ à¤•à¤¿à¤¯à¤¾ à¤œà¤¾à¤¤à¤¾ à¤¹à¥ˆ?\n",
      "   6. à¤œà¤¼à¥‹à¤²à¥‡à¤¡à¥à¤°à¥‹à¤¨à¤¿à¤• à¤à¤¸à¤¿à¤¡ à¤‡à¤‚à¤œà¥‡à¤•à¥à¤¶à¤¨ à¤•à¥‡ à¤¸à¤‚à¤­à¤¾à¤µà¤¿à¤¤ à¤—à¥à¤°à¥à¤¦à¥‡ à¤¸à¤‚à¤¬à¤‚à¤§à¥€ à¤¦à¥à¤·à¥à¤ªà¥à¤°à¤­à¤¾à¤µ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆà¤‚?\n",
      "   7. à¤®à¥‡à¤ªà¥‡à¤°à¤¿à¤¡à¤¾à¤‡à¤¨ à¤¹à¤¾à¤‡à¤¡à¥à¤°à¥‹à¤•à¥à¤²à¥‹à¤°à¤¾à¤‡à¤¡ à¤•à¤¾ à¤¸à¤‚à¤°à¤šà¤¨à¤¾à¤¤à¥à¤®à¤• à¤¸à¥‚à¤¤à¥à¤° à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ?\n",
      "   8. à¤¹à¥ƒà¤¦à¤¯ à¤°à¥‹à¤— à¤•à¥‡ à¤•à¥Œà¤¨ à¤¸à¥‡ à¤œà¥‹à¤–à¤¿à¤® à¤•à¤¾à¤°à¤• à¤¬à¤¦à¤²à¥‡ à¤œà¤¾ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚?\n",
      "   9. à¤¤à¤¨à¤¾à¤µ à¤ªà¥à¤°à¤¬à¤‚à¤§à¤¨ à¤¹à¥ƒà¤¦à¤¯ à¤°à¥‹à¤— à¤•à¥‡ à¤œà¥‹à¤–à¤¿à¤® à¤•à¥‹ à¤•à¥ˆà¤¸à¥‡ à¤ªà¥à¤°à¤­à¤¾à¤µà¤¿à¤¤ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ?\n",
      "  10. à¤¸à¥à¤¥à¤¾à¤¨à¥€à¤¯ à¤à¤¨à¥‡à¤¸à¥à¤¥à¥‡à¤Ÿà¤¿à¤•à¥à¤¸ à¤•à¥€ à¤•à¥à¤°à¤¿à¤¯à¤¾ à¤®à¥‡à¤‚ à¤ªà¥à¤²à¤¾à¤œà¥à¤®à¤¾ à¤ªà¥à¤°à¥‹à¤Ÿà¥€à¤¨ à¤•à¥€ à¤•à¥à¤¯à¤¾ à¤­à¥‚à¤®à¤¿à¤•à¤¾ à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆ?\n",
      "  11. à¤µà¤¿à¤­à¤¿à¤¨à¥à¤¨ à¤œà¤¾à¤¤à¥€à¤¯ à¤¸à¤®à¥‚à¤¹à¥‹à¤‚ à¤®à¥‡à¤‚ à¤¹à¥ƒà¤¦à¤¯ à¤°à¥‹à¤— à¤•à¥€ à¤¦à¤°à¥‹à¤‚ à¤®à¥‡à¤‚ à¤•à¥à¤¯à¤¾ à¤…à¤‚à¤¤à¤° à¤¹à¥ˆà¤‚?\n",
      "  12. à¤¶à¤°à¤¾à¤¬ à¤•à¥‡ à¤¸à¥‡à¤µà¤¨ à¤•à¤¾ à¤¹à¥ƒà¤¦à¤¯ à¤°à¥‹à¤— à¤•à¥‡ à¤œà¥‹à¤–à¤¿à¤® à¤ªà¤° à¤•à¥à¤¯à¤¾ à¤ªà¥à¤°à¤­à¤¾à¤µ à¤ªà¤¡à¤¼à¤¤à¤¾ à¤¹à¥ˆ?\n",
      "  13. à¤§à¥‚à¤®à¥à¤°à¤ªà¤¾à¤¨ à¤•à¤°à¤¨à¥‡ à¤µà¤¾à¤²à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤¹à¥ƒà¤¦à¤¯ à¤°à¥‹à¤— à¤•à¥‡ à¤œà¥‹à¤–à¤¿à¤® à¤•à¥‹ à¤•à¤® à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤•à¥à¤¯à¤¾ à¤…à¤¨à¥à¤¶à¤‚à¤¸à¤¿à¤¤ à¤•à¤¾à¤°à¥à¤°à¤µà¤¾à¤ˆ à¤¹à¥ˆ?\n",
      "  14. à¤•à¥Œà¤¨ à¤¸à¥€ à¤®à¥‡à¤ªà¤¿à¤µà¤¾à¤•à¥‡à¤¨ à¤¹à¤¾à¤‡à¤¡à¥à¤°à¥‹à¤•à¥à¤²à¥‹à¤°à¤¾à¤‡à¤¡ à¤•à¥€ à¤¸à¤¾à¤‚à¤¦à¥à¤°à¤¤à¤¾ à¤ªà¥‚à¤°à¥à¤£ à¤¸à¤‚à¤µà¥‡à¤¦à¥€ à¤”à¤° à¤®à¥‹à¤Ÿà¤° à¤¬à¥à¤²à¥‰à¤• à¤ªà¥à¤°à¤¦à¤¾à¤¨ à¤•à¤°à¤¤à¥€ à¤¹à¥ˆ?\n",
      "  15. à¤®à¥‡à¤ªà¤¿à¤µà¤¾à¤•à¥‡à¤¨ à¤¦à¥à¤µà¤¾à¤°à¤¾ à¤ªà¥à¤°à¤¦à¤¾à¤¨ à¤•à¥€ à¤—à¤ˆ à¤à¤¨à¥‡à¤¸à¥à¤¥à¥€à¤¸à¤¿à¤¯à¤¾ à¤†à¤®à¤¤à¥Œà¤° à¤ªà¤° à¤¸à¤°à¥à¤œà¤°à¥€ à¤•à¥‡ à¤²à¤¿à¤ à¤•à¤¿à¤¤à¤¨à¥€ à¤¦à¥‡à¤° à¤¤à¤• à¤ªà¤°à¥à¤¯à¤¾à¤ªà¥à¤¤ à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆ?\n",
      "  16. à¤œà¤¼à¥‹à¤²à¥‡à¤¡à¥à¤°à¥‹à¤¨à¤¿à¤• à¤à¤¸à¤¿à¤¡ à¤‡à¤‚à¤œà¥‡à¤•à¥à¤¶à¤¨ à¤•à¥‡ à¤ªà¥‹à¤¸à¥à¤Ÿ-à¤…à¤ªà¥à¤°à¥‚à¤µà¤² à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¥‡ à¤¦à¥Œà¤°à¤¾à¤¨ à¤°à¤¿à¤ªà¥‹à¤°à¥à¤Ÿ à¤•à¥€ à¤—à¤ˆ à¤ªà¥à¤°à¤¤à¤¿à¤•à¥‚à¤² à¤ªà¥à¤°à¤¤à¤¿à¤•à¥à¤°à¤¿à¤¯à¤¾à¤à¤‚ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆà¤‚?\n",
      "  17. à¤®à¥‡à¤ªà¥‡à¤°à¤¿à¤¡à¤¾à¤‡à¤¨ à¤¹à¤¾à¤‡à¤¡à¥à¤°à¥‹à¤•à¥à¤²à¥‹à¤°à¤¾à¤‡à¤¡ à¤‡à¤‚à¤œà¥‡à¤•à¥à¤¶à¤¨ à¤•à¤¾ à¤ªà¥€à¤à¤š à¤°à¥‡à¤‚à¤œ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ?\n",
      "  18. à¤…à¤¤à¤¿à¤°à¤¿à¤•à¥à¤¤ à¤•à¥ˆà¤²à¥‹à¤°à¥€ à¤•à¤¾ à¤¹à¥ƒà¤¦à¤¯ à¤°à¥‹à¤— à¤•à¥‡ à¤œà¥‹à¤–à¤¿à¤® à¤ªà¤° à¤•à¥à¤¯à¤¾ à¤ªà¥à¤°à¤­à¤¾à¤µ à¤ªà¤¡à¤¼à¤¤à¤¾ à¤¹à¥ˆ?\n",
      "  19. à¤®à¥‡à¤ªà¥‡à¤°à¤¿à¤¡à¤¾à¤‡à¤¨ à¤¹à¤¾à¤‡à¤¡à¥à¤°à¥‹à¤•à¥à¤²à¥‹à¤°à¤¾à¤‡à¤¡ à¤‡à¤‚à¤œà¥‡à¤•à¥à¤¶à¤¨ à¤•à¥‡ à¤ªà¥à¤°à¤¶à¤¾à¤¸à¤¨ à¤•à¥‡ à¤°à¥‚à¤ª à¤•à¥à¤¯à¤¾ à¤¹à¥ˆà¤‚?\n",
      "  20. à¤®à¥‡à¤ªà¥‡à¤°à¤¿à¤¡à¤¾à¤‡à¤¨ à¤¹à¤¾à¤‡à¤¡à¥à¤°à¥‹à¤•à¥à¤²à¥‹à¤°à¤¾à¤‡à¤¡ à¤•à¤¾ à¤—à¤²à¤¨à¤¾à¤‚à¤• à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ?\n",
      "  21. à¤§à¥‚à¤®à¥à¤°à¤ªà¤¾à¤¨ à¤›à¥‹à¤¡à¤¼à¤¨à¥‡ à¤•à¤¾ à¤¹à¥ƒà¤¦à¤¯ à¤°à¥‹à¤— à¤•à¥‡ à¤œà¥‹à¤–à¤¿à¤® à¤ªà¤° à¤•à¥à¤¯à¤¾ à¤ªà¥à¤°à¤­à¤¾à¤µ à¤ªà¤¡à¤¼à¤¤à¤¾ à¤¹à¥ˆ?\n",
      "  22. à¤¤à¤¨à¤¾à¤µ à¤•à¤¾ à¤¹à¥ƒà¤¦à¤¯ à¤°à¥‹à¤— à¤¸à¥‡ à¤•à¥à¤¯à¤¾ à¤¸à¤‚à¤¬à¤‚à¤§ à¤¹à¥ˆ?\n",
      "  23. à¤®à¥‡à¤ªà¥‡à¤°à¤¿à¤¡à¤¾à¤‡à¤¨ à¤¹à¤¾à¤‡à¤¡à¥à¤°à¥‹à¤•à¥à¤²à¥‹à¤°à¤¾à¤‡à¤¡ à¤•à¥€ à¤°à¤¾à¤¸à¤¾à¤¯à¤¨à¤¿à¤• à¤¸à¤‚à¤°à¤šà¤¨à¤¾ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ?\n",
      "  24. à¤•à¥Œà¤¨ à¤¸à¤¾ à¤œà¤¾à¤¤à¥€à¤¯ à¤¸à¤®à¥‚à¤¹ à¤…à¤¨à¥à¤¯ à¤•à¥€ à¤¤à¥à¤²à¤¨à¤¾ à¤®à¥‡à¤‚ à¤¹à¥ƒà¤¦à¤¯ à¤°à¥‹à¤— à¤•à¥€ à¤¦à¤°à¥‹à¤‚ à¤®à¥‡à¤‚ à¤•à¤® à¤¹à¥ˆ?\n",
      "  25. à¤®à¤ªà¥‡à¤°à¥€à¤¡à¤¾à¤‡à¤¨ à¤¹à¤¾à¤‡à¤¡à¥à¤°à¥‹à¤•à¥à¤²à¥‹à¤°à¤¾à¤‡à¤¡ à¤•à¤¾ à¤¨à¤¾à¤°à¤•à¥‹à¤Ÿà¤¿à¤• à¤à¤¨à¤¾à¤²à¥à¤œà¥‡à¤¸à¤¿à¤• à¤•à¥‡ à¤°à¥‚à¤ª à¤®à¥‡à¤‚ à¤•à¥à¤¯à¤¾ à¤­à¥‚à¤®à¤¿à¤•à¤¾ à¤¹à¥ˆ?\n",
      "  26. à¤¹à¥ƒà¤¦à¤¯ à¤°à¥‹à¤— à¤•à¥‡ à¤œà¥‹à¤–à¤¿à¤® à¤•à¥‹ à¤•à¤® à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤®à¤¹à¤¿à¤²à¤¾à¤“à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤¶à¤°à¤¾à¤¬ à¤¸à¥‡à¤µà¤¨ à¤•à¥‡ à¤¦à¤¿à¤¶à¤¾-à¤¨à¤¿à¤°à¥à¤¦à¥‡à¤¶ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆà¤‚?\n",
      "  27. à¤ªà¤°à¤¿à¤µà¤¾à¤° à¤•à¥‡ à¤¸à¤¦à¤¸à¥à¤¯à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤¹à¥ƒà¤¦à¤¯ à¤°à¥‹à¤— à¤¨à¤¿à¤¦à¤¾à¤¨ à¤•à¥€ à¤†à¤¯à¥ à¤¸à¥€à¤®à¤¾à¤à¤‚ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆà¤‚ à¤œà¥‹ à¤œà¥‹à¤–à¤¿à¤® à¤¬à¤¢à¤¼à¤¾à¤¤à¥€ à¤¹à¥ˆà¤‚?\n",
      "  28. à¤®à¥‡à¤ªà¥‡à¤°à¤¿à¤¡à¤¾à¤‡à¤¨ à¤¹à¤¾à¤‡à¤¡à¥à¤°à¥‹à¤•à¥à¤²à¥‹à¤°à¤¾à¤‡à¤¡ à¤‡à¤‚à¤œà¥‡à¤•à¥à¤¶à¤¨ à¤•à¥‡ à¤²à¤¿à¤ à¤‰à¤ªà¤²à¤¬à¥à¤§ à¤¸à¤¾à¤‚à¤¦à¥à¤°à¤¤à¤¾ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆà¤‚?\n",
      "  29. à¤¸à¥à¤¥à¤¾à¤¨à¥€à¤¯ à¤à¤¨à¥‡à¤¸à¥à¤¥à¥‡à¤Ÿà¤¿à¤•à¥à¤¸ à¤®à¥‡à¤‚ à¤ªà¥à¤²à¤¾à¤œà¥à¤®à¤¾ à¤¸à¤¾à¤‚à¤¦à¥à¤°à¤¤à¤¾ à¤”à¤° à¤¦à¤µà¤¾ à¤¬à¤¾à¤‡à¤‚à¤¡à¤¿à¤‚à¤— à¤•à¥‡ à¤¬à¥€à¤š à¤•à¥à¤¯à¤¾ à¤¸à¤‚à¤¬à¤‚à¤§ à¤¹à¥ˆ?\n",
      "  30. à¤…à¤¨à¥à¤¶à¤‚à¤¸à¤¿à¤¤ à¤¶à¤°à¤¾à¤¬ à¤¸à¥‡ à¤…à¤§à¤¿à¤• à¤ªà¥€à¤¨à¥‡ à¤•à¥‡ à¤¹à¥ƒà¤¦à¤¯ à¤°à¥‹à¤— à¤œà¥‹à¤–à¤¿à¤® à¤ªà¤° à¤•à¥à¤¯à¤¾ à¤ªà¥à¤°à¤­à¤¾à¤µ à¤¹à¥‹à¤¤à¥‡ à¤¹à¥ˆà¤‚?\n",
      "\n",
      "Completed Hindi translations: 30\n",
      "\n",
      "Translating questions to Chinese...\n",
      "   1. ä»€ä¹ˆæ˜¯èƒ°å²›ç´ æŠµæŠ—ï¼Œå®ƒå¦‚ä½•å½±å“è‘¡è„ç³–è¿›å…¥ç»†èƒžï¼Ÿ\n",
      "   2. å®¶æ—å²å¦‚ä½•å½±å“å¿ƒè„ç—…çš„é£Žé™©ï¼Ÿ\n",
      "   3. ç”·æ€§å’Œå¥³æ€§å‡å°‘å¿ƒè„ç—…é£Žé™©çš„é¥®é…’æŒ‡å—æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "   4. å¸çƒŸå¯¹è¡€åŽ‹å’Œå¿ƒè„ç—…é£Žé™©æœ‰ä½•å½±å“ï¼Ÿ\n",
      "   5. ç›é…¸ç”²å“Œå¡å› ç”¨äºŽä»€ä¹ˆï¼Ÿ\n",
      "   6. å”‘æ¥è†¦é…¸æ³¨å°„æ¶²çš„æ½œåœ¨è‚¾è„å‰¯ä½œç”¨æœ‰å“ªäº›ï¼Ÿ\n",
      "   7. å“Œæ›¿å•¶ç›é…¸ç›çš„ç»“æž„å¼æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "   8. å¯ä»¥æ”¹å˜çš„å¿ƒè„ç—…é£Žé™©å› ç´ æœ‰å“ªäº›ï¼Ÿ\n",
      "   9. ç®¡ç†åŽ‹åŠ›å¦‚ä½•å½±å“å¿ƒè„ç—…é£Žé™©ï¼Ÿ\n",
      "  10. è¡€æµ†è›‹ç™½åœ¨å±€éƒ¨éº»é†‰è¯ä½œç”¨ä¸­çš„è§’è‰²æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "  11. ä¸åŒæ—ç¾¤ä¹‹é—´å¿ƒè„ç—…å‘ç—…çŽ‡æœ‰ä»€ä¹ˆå·®å¼‚ï¼Ÿ\n",
      "  12. é…’ç²¾æ‘„å…¥å¯¹å¿ƒè„ç—…é£Žé™©æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ\n",
      "  13. å¸çƒŸè€…é™ä½Žå¿ƒè„ç—…é£Žé™©çš„å»ºè®®æŽªæ–½æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "  14. ä»€ä¹ˆæµ“åº¦çš„ç›é…¸ç”²å“Œå¡å› å¯ä»¥æä¾›å®Œå…¨çš„æ„Ÿè§‰å’Œè¿åŠ¨é˜»æ»žï¼Ÿ\n",
      "  15. ç›é…¸ç”²å“Œå¡å› æä¾›çš„éº»é†‰é€šå¸¸èƒ½ç»´æŒæ‰‹æœ¯å¤šé•¿æ—¶é—´ï¼Ÿ\n",
      "  16. åœ¨å”‘æ¥è†¦é…¸æ³¨å°„æ¶²çš„ä¸Šå¸‚åŽä½¿ç”¨ä¸­æŠ¥å‘Šäº†å“ªäº›ä¸è‰¯ååº”ï¼Ÿ\n",
      "  17. å“Œæ›¿å•¶ç›é…¸ç›æ³¨å°„æ¶²çš„pHèŒƒå›´æ˜¯å¤šå°‘ï¼Ÿ\n",
      "  18. é¢å¤–çš„å¡è·¯é‡Œå¯¹å¿ƒè„ç—…é£Žé™©æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ\n",
      "  19. å“Œæ›¿å•¶ç›é…¸ç›æ³¨å°„æ¶²æœ‰å“ªäº›ç»™è¯å½¢å¼ï¼Ÿ\n",
      "  20. å“Œæ›¿å•¶ç›é…¸ç›çš„ç†”ç‚¹æ˜¯å¤šå°‘ï¼Ÿ\n",
      "  21. æˆ’çƒŸå¯¹å¿ƒè„ç—…é£Žé™©æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ\n",
      "  22. åŽ‹åŠ›ä¸Žå¿ƒè„ç—…æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ\n",
      "  23. ç›é…¸å“Œæ›¿å•¶çš„åŒ–å­¦æˆåˆ†æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "  24. å“ªä¸ªæ—ç¾¤çš„å¿ƒè„ç—…å‘ç—…çŽ‡æ¯”å…¶ä»–æ—ç¾¤ä½Žï¼Ÿ\n",
      "  25. ç›é…¸å“Œæ›¿å•¶ä½œä¸ºéº»é†‰æ€§é•‡ç—›è¯çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "  26. å¥³æ€§é¥®é…’ä»¥é™ä½Žå¿ƒè„ç—…é£Žé™©çš„æŒ‡å—æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "  27. å®¶æ—æˆå‘˜å¿ƒè„ç—…è¯Šæ–­çš„å¹´é¾„é˜ˆå€¼æ˜¯å¤šå°‘ä¼šå¢žåŠ é£Žé™©ï¼Ÿ\n",
      "  28. ç›é…¸å“Œæ›¿å•¶æ³¨å°„æ¶²æœ‰å“ªäº›æµ“åº¦å¯ç”¨ï¼Ÿ\n",
      "  29. è¡€æµ†æµ“åº¦ä¸Žå±€éƒ¨éº»é†‰è¯ç‰©ç»“åˆä¹‹é—´çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "  30. é¥®é…’è¶…è¿‡æŽ¨èé‡å¯¹å¿ƒè„ç—…é£Žé™©æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ\n",
      "Completed Chinese translations: 30\n",
      "\n",
      "================================================================================\n",
      "STEP 4: ORGANIZING BY CATEGORY AND SAVING\n",
      "================================================================================\n",
      "\n",
      "Saved questions to:\n",
      "  Hindi: C:\\Users\\Boris\\Desktop\\code\\multilingual-rag\\data\\questions\\hindi_questions.json\n",
      "  Chinese: C:\\Users\\Boris\\Desktop\\code\\multilingual-rag\\data\\questions\\chinese_questions.json\n",
      "\n",
      "================================================================================\n",
      "VERIFICATION: ENSURING IDENTICAL QUESTION SETS\n",
      "================================================================================\n",
      "\n",
      "Hindi questions by category:\n",
      "  condition: 11 questions\n",
      "  treatment: 5 questions\n",
      "  medication: 12 questions\n",
      "  side_effect: 2 questions\n",
      "\n",
      "Chinese questions by category:\n",
      "  condition: 11 questions\n",
      "  treatment: 5 questions\n",
      "  medication: 12 questions\n",
      "  side_effect: 2 questions\n",
      "\n",
      "Verifying identical English base:\n",
      "âœ… SUCCESS: Both language sets have identical English questions\n",
      "   Total unique English questions: 30\n",
      "\n",
      "Sample question comparison:\n",
      "================================================================================\n",
      "English: What is insulin resistance and how does it affect glucose entry into cells?\n",
      "Hindi:   à¤‡à¤‚à¤¸à¥à¤²à¤¿à¤¨ à¤ªà¥à¤°à¤¤à¤¿à¤°à¥‹à¤§ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ à¤”à¤° à¤¯à¤¹ à¤•à¥‹à¤¶à¤¿à¤•à¤¾à¤“à¤‚ à¤®à¥‡à¤‚ à¤—à¥à¤²à¥‚à¤•à¥‹à¤œ à¤•à¥‡ à¤ªà¥à¤°à¤µà¥‡à¤¶ à¤•à¥‹ à¤•à¥ˆà¤¸à¥‡ à¤ªà¥à¤°à¤­à¤¾à¤µà¤¿à¤¤ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ?\n",
      "Chinese: ä»€ä¹ˆæ˜¯èƒ°å²›ç´ æŠµæŠ—ï¼Œå®ƒå¦‚ä½•å½±å“è‘¡è„ç³–è¿›å…¥ç»†èƒžï¼Ÿ\n",
      "Category: condition | Complexity: moderate\n",
      "\n",
      "================================================================================\n",
      "FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "âœ… Generated identical question sets for apples-to-apples comparison\n",
      "   Total questions per language: 30\n",
      "   Languages: Hindi, Chinese\n",
      "   Base: Same English questions translated to both languages\n",
      "\n",
      "Breakdown by category:\n",
      "  condition: 11 Hindi, 11 Chinese\n",
      "  medication: 12 Hindi, 12 Chinese\n",
      "  side_effect: 2 Hindi, 2 Chinese\n",
      "  treatment: 5 Hindi, 5 Chinese\n",
      "\n",
      "================================================================================\n",
      "READY FOR NOTEBOOK 3!\n",
      "================================================================================\n",
      "\n",
      "Next steps:\n",
      "1. Run notebook 3 to load these identical question sets\n",
      "2. Compare Hindi vs Chinese performance on SAME questions\n",
      "3. Ensure true apples-to-apples comparison across languages\n",
      "4. Results will show language-specific differences, not question difficulty differences\n",
      "\n",
      "ðŸ“‹ Saved master comparison file: C:\\Users\\Boris\\Desktop\\code\\multilingual-rag\\data\\questions\\question_comparison.json\n",
      "   This file shows all questions side-by-side for verification\n"
     ]
    }
   ],
   "source": [
    "# Notebook 2x: Generate IDENTICAL Questions for Hindi and Chinese - Apples-to-Apples Comparison\n",
    "# Run this BEFORE notebook 3 to create identical questions in both languages\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict\n",
    "import openai\n",
    "from pprint import pformat\n",
    "\n",
    "# API KEY\n",
    "OPEN_API_KEY = \"YOUR KEY\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPEN_API_KEY\n",
    "openai.api_key = OPEN_API_KEY\n",
    "\n",
    "# Correct v1 client\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=OPEN_API_KEY)\n",
    "\n",
    "# Paths\n",
    "ROOT_DIR = Path('..').resolve()\n",
    "RAW_DATA_DIR = ROOT_DIR / 'data' / 'raw'\n",
    "PROCESSED_DATA_DIR = ROOT_DIR / 'data' / 'processed'\n",
    "QUESTIONS_DIR = ROOT_DIR / 'data' / 'questions'\n",
    "QUESTIONS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GENERATING IDENTICAL QUESTIONS FOR HINDI AND CHINESE\")\n",
    "print(\"Strategy: Generate English questions, then translate to both languages\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load processed documents\n",
    "with open(PROCESSED_DATA_DIR / 'processed_documents.json', 'r', encoding='utf-8') as f:\n",
    "    docs_data = json.load(f)\n",
    "\n",
    "print(f\"\\nTotal documents: {len(docs_data)}\")\n",
    "\n",
    "# Analyze by source\n",
    "sources = Counter([doc['metadata']['source'] for doc in docs_data])\n",
    "print(f\"\\nDocuments by source:\")\n",
    "for source, count in sources.items():\n",
    "    print(f\"  {source}: {count} documents\")\n",
    "\n",
    "# Sample content from each source\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CORPUS ANALYSIS FOR QUESTION GENERATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for source in sources.keys():\n",
    "    source_docs = [doc for doc in docs_data if doc['metadata']['source'] == source]\n",
    "    if not source_docs:\n",
    "        continue\n",
    "    print(f\"\\n{source} - Sample content:\")\n",
    "    print((source_docs[0].get('content') or \"\")[:500] + \"...\")\n",
    "\n",
    "# Generate base English questions\n",
    "random.seed(42)\n",
    "\n",
    "def _sample_corpus(docs_data, per_source=3, max_sources=4, clip_chars=600):\n",
    "    \"\"\"Small, diverse, token-safe corpus sample.\"\"\"\n",
    "    uniq_sources = list({doc[\"metadata\"][\"source\"] for doc in docs_data})\n",
    "    random.shuffle(uniq_sources)\n",
    "    uniq_sources = uniq_sources[:max_sources]\n",
    "\n",
    "    samples = []\n",
    "    for src in uniq_sources:\n",
    "        source_docs = [d for d in docs_data if d[\"metadata\"][\"source\"] == src]\n",
    "        if not source_docs:\n",
    "            continue\n",
    "        random.shuffle(source_docs)\n",
    "        take = min(per_source, len(source_docs))\n",
    "        for d in source_docs[:take]:\n",
    "            content = (d.get(\"content\") or \"\")[:clip_chars]\n",
    "            samples.append({\"source\": src, \"content\": content})\n",
    "    return samples\n",
    "\n",
    "def _normalize_category(cat: str) -> str:\n",
    "    c = (cat or \"\").strip().lower()\n",
    "    if c in {\"medication\",\"condition\",\"treatment\",\"side_effect\"}:\n",
    "        return c\n",
    "    if \"side\" in c: return \"side_effect\"\n",
    "    if \"treat\" in c: return \"treatment\"\n",
    "    if \"cond\" in c: return \"condition\"\n",
    "    return \"medication\"\n",
    "\n",
    "def _normalize_source(src: str) -> str:\n",
    "    s = (src or \"\").lower()\n",
    "    if \"cdc\" in s: return \"CDC\"\n",
    "    if \"medline\" in s or \"nih\" in s: return \"MedlinePlus (NIH)\"\n",
    "    return \"FDA\"\n",
    "\n",
    "def generate_base_english_questions(docs_data: List[Dict], num_questions: int = 30) -> List[Dict]:\n",
    "    \"\"\"Generate base English questions that will be translated to both languages\"\"\"\n",
    "    samples = _sample_corpus(docs_data, per_source=3, max_sources=4, clip_chars=600)\n",
    "    if not samples:\n",
    "        print(\"[Question gen] No samples found; returning empty list.\")\n",
    "        return []\n",
    "\n",
    "    corpus_context = \"\\n\\n---SAMPLE FROM CORPUS---\\n\\n\".join(\n",
    "        [f\"Source: {s['source']}\\n{s['content']}\" for s in samples]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"You are a medical question generator. Based ONLY on the corpus samples below, generate EXACTLY {num_questions} English medical questions that CAN BE ANSWERED from these samples.\n",
    "\n",
    "CORPUS SAMPLES:\n",
    "{corpus_context}\n",
    "\n",
    "RULES:\n",
    "1) Every question must be answerable using the samples above (no outside knowledge).\n",
    "2) Cover medications, conditions, treatments, and side effects actually present.\n",
    "3) Generate questions in ENGLISH only (we will translate them later).\n",
    "4) category âˆˆ [\"medication\",\"condition\",\"treatment\",\"side_effect\"].\n",
    "5) complexity âˆˆ [\"simple\",\"moderate\",\"complex\"].\n",
    "6) expected_source âˆˆ [\"FDA\",\"CDC\",\"MedlinePlus (NIH)\"] chosen from the samples' sources.\n",
    "7) Respond ONLY as a JSON OBJECT with a top-level key \"questions\" whose value is a JSON ARRAY of exactly {num_questions} items.\n",
    "8) No extra keys or commentary outside \"questions\".\n",
    "\n",
    "JSON SHAPE:\n",
    "{{\n",
    "  \"questions\": [\n",
    "    {{\n",
    "      \"english\": \"string\",\n",
    "      \"category\": \"medication|condition|treatment|side_effect\",\n",
    "      \"complexity\": \"simple|moderate|complex\",\n",
    "      \"expected_source\": \"FDA|CDC|MedlinePlus (NIH)\",\n",
    "      \"answerable\": true\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    raw = None\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You generate answerable medical questions strictly grounded in the provided corpus.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.5,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        raw = resp.choices[0].message.content\n",
    "        data = json.loads(raw)\n",
    "        qs = data.get(\"questions\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"[Question gen] Error parsing json_object: {e}\")\n",
    "        try:\n",
    "            resp2 = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Output ONLY valid JSON per the user instruction.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.5,\n",
    "            )\n",
    "            raw = resp2.choices[0].message.content\n",
    "            m = re.search(r\"\\{.*\\}\", raw, flags=re.S)\n",
    "            data = json.loads(m.group(0)) if m else json.loads(raw)\n",
    "            qs = data.get(\"questions\", data if isinstance(data, list) else [])\n",
    "        except Exception as e2:\n",
    "            print(f\"[Question gen] Fallback parse failed: {e2}\")\n",
    "            if raw:\n",
    "                print(\"[Assistant raw output]:\\n\", raw[:2000])\n",
    "            return []\n",
    "\n",
    "    out = []\n",
    "    for q in qs:\n",
    "        if not isinstance(q, dict):\n",
    "            continue\n",
    "        en = (q.get(\"english\") or \"\").strip()\n",
    "        if not en:\n",
    "            continue\n",
    "        out.append({\n",
    "            \"english\": en,\n",
    "            \"category\": _normalize_category(q.get(\"category\")),\n",
    "            \"complexity\": (q.get(\"complexity\") or \"simple\").strip().lower(),\n",
    "            \"expected_source\": _normalize_source(q.get(\"expected_source\")),\n",
    "            \"answerable\": True\n",
    "        })\n",
    "\n",
    "    return out[:num_questions]\n",
    "\n",
    "def translate_questions_to_language(questions: List[Dict], target_language: str, language_name: str) -> List[Dict]:\n",
    "    \"\"\"Translate English questions to target language\"\"\"\n",
    "    print(f\"\\nTranslating questions to {language_name}...\")\n",
    "    \n",
    "    translated_questions = []\n",
    "    \n",
    "    # Process in batches of 5 for efficiency\n",
    "    batch_size = 5\n",
    "    for i in range(0, len(questions), batch_size):\n",
    "        batch = questions[i:i+batch_size]\n",
    "        \n",
    "        # Prepare batch for translation\n",
    "        batch_text = \"\\n\".join([f\"{j+1}. {q['english']}\" for j, q in enumerate(batch)])\n",
    "        \n",
    "        prompt = f\"\"\"Translate the following medical questions to {language_name}. Maintain medical accuracy and natural fluency.\n",
    "\n",
    "English Questions:\n",
    "{batch_text}\n",
    "\n",
    "Instructions:\n",
    "1. Translate each question accurately to {language_name}\n",
    "2. Maintain the medical terminology appropriately\n",
    "3. Keep the same question numbering\n",
    "4. Respond with ONLY the translated questions, one per line, with numbers\n",
    "\n",
    "Example format:\n",
    "1. [Translated question 1]\n",
    "2. [Translated question 2]\n",
    "etc.\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": f\"You are an expert medical translator for {language_name}. Translate accurately while maintaining medical precision.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "            )\n",
    "            \n",
    "            translated_text = resp.choices[0].message.content\n",
    "            translated_lines = [line.strip() for line in translated_text.split('\\n') if line.strip()]\n",
    "            \n",
    "            # Extract translations\n",
    "            for j, q in enumerate(batch):\n",
    "                if j < len(translated_lines):\n",
    "                    # Remove numbering (e.g., \"1. \" from the start)\n",
    "                    translation = re.sub(r'^\\d+\\.\\s*', '', translated_lines[j])\n",
    "                    \n",
    "                    translated_q = q.copy()\n",
    "                    translated_q[target_language] = translation\n",
    "                    translated_questions.append(translated_q)\n",
    "                    \n",
    "                    print(f\"  {i+j+1:2d}. {translation}\")\n",
    "                else:\n",
    "                    # Fallback if translation failed\n",
    "                    translated_q = q.copy()\n",
    "                    translated_q[target_language] = f\"[Translation failed for: {q['english']}]\"\n",
    "                    translated_questions.append(translated_q)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error translating batch {i//batch_size + 1}: {e}\")\n",
    "            # Add fallback entries\n",
    "            for q in batch:\n",
    "                translated_q = q.copy()\n",
    "                translated_q[target_language] = f\"[Translation failed for: {q['english']}]\"\n",
    "                translated_questions.append(translated_q)\n",
    "    \n",
    "    return translated_questions\n",
    "\n",
    "# Validate questions against corpus\n",
    "def validate_question_answerability(question_english: str, docs_data: List[Dict]) -> Dict:\n",
    "    \"\"\"Check if a question can likely be answered from the corpus\"\"\"\n",
    "    key_terms = re.findall(r'\\b\\w+\\b', question_english.lower())\n",
    "    key_terms = [t for t in key_terms if len(t) > 4]\n",
    "\n",
    "    relevant_docs = []\n",
    "    for doc in docs_data:\n",
    "        content_lower = (doc.get('content') or \"\").lower()\n",
    "        matches = sum(1 for term in key_terms if term in content_lower)\n",
    "        if matches >= 2:\n",
    "            relevant_docs.append({\n",
    "                'content': (doc.get('content') or \"\")[:300],\n",
    "                'source': doc['metadata']['source'],\n",
    "                'match_count': matches\n",
    "            })\n",
    "\n",
    "    return {\n",
    "        'answerable': len(relevant_docs) > 0,\n",
    "        'relevant_docs_count': len(relevant_docs),\n",
    "        'top_docs': sorted(relevant_docs, key=lambda x: x['match_count'], reverse=True)[:3]\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: GENERATING BASE ENGLISH QUESTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Generate base English questions\n",
    "base_questions = generate_base_english_questions(docs_data, num_questions=30)\n",
    "print(f\"Generated {len(base_questions)} base English questions\")\n",
    "\n",
    "# Show samples\n",
    "print(f\"\\nSample English questions:\")\n",
    "for i, q in enumerate(base_questions[:5], 1):\n",
    "    print(f\"  {i}. {q['english']}\")\n",
    "    print(f\"     Category: {q['category']} | Complexity: {q['complexity']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: VALIDATING QUESTIONS AGAINST CORPUS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Validate all questions\n",
    "validated_questions = []\n",
    "for q in base_questions:\n",
    "    validation = validate_question_answerability(q['english'], docs_data)\n",
    "    q['validation'] = validation\n",
    "    if validation['answerable']:\n",
    "        validated_questions.append(q)\n",
    "\n",
    "print(f\"Answerable questions: {len(validated_questions)}/{len(base_questions)}\")\n",
    "\n",
    "# If we don't have enough, take the best ones we have\n",
    "if len(validated_questions) < 30:\n",
    "    print(f\"Warning: Only {len(validated_questions)} questions are fully answerable from corpus\")\n",
    "    print(\"Using all available validated questions\")\n",
    "    final_questions = validated_questions\n",
    "else:\n",
    "    # Take exactly 30\n",
    "    final_questions = validated_questions[:30]\n",
    "\n",
    "print(f\"Final question set: {len(final_questions)} questions\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: TRANSLATING TO HINDI AND CHINESE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Translate to Hindi\n",
    "hindi_questions = translate_questions_to_language(final_questions, \"hindi\", \"Hindi\")\n",
    "print(f\"\\nCompleted Hindi translations: {len(hindi_questions)}\")\n",
    "\n",
    "# Translate to Chinese\n",
    "chinese_questions = translate_questions_to_language(final_questions, \"chinese\", \"Chinese\")\n",
    "print(f\"Completed Chinese translations: {len(chinese_questions)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: ORGANIZING BY CATEGORY AND SAVING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Group by category for both languages\n",
    "def group_by_category(questions):\n",
    "    grouped = defaultdict(list)\n",
    "    for q in questions:\n",
    "        grouped[q['category']].append(q)\n",
    "    return dict(grouped)\n",
    "\n",
    "hindi_grouped = group_by_category(hindi_questions)\n",
    "chinese_grouped = group_by_category(chinese_questions)\n",
    "\n",
    "# Save to JSON files (NO TIMESTAMPS)\n",
    "hindi_file = QUESTIONS_DIR / 'hindi_questions.json'\n",
    "chinese_file = QUESTIONS_DIR / 'chinese_questions.json'\n",
    "\n",
    "with open(hindi_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(hindi_grouped, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(chinese_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chinese_grouped, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nSaved questions to:\")\n",
    "print(f\"  Hindi: {hindi_file}\")\n",
    "print(f\"  Chinese: {chinese_file}\")\n",
    "\n",
    "# Verification: Show that questions are identical (same English base)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICATION: ENSURING IDENTICAL QUESTION SETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nHindi questions by category:\")\n",
    "for category, questions in hindi_grouped.items():\n",
    "    print(f\"  {category}: {len(questions)} questions\")\n",
    "\n",
    "print(f\"\\nChinese questions by category:\")\n",
    "for category, questions in chinese_grouped.items():\n",
    "    print(f\"  {category}: {len(questions)} questions\")\n",
    "\n",
    "# Verify same English questions\n",
    "print(f\"\\nVerifying identical English base:\")\n",
    "hindi_english = set()\n",
    "chinese_english = set()\n",
    "\n",
    "for questions in hindi_grouped.values():\n",
    "    for q in questions:\n",
    "        hindi_english.add(q['english'])\n",
    "\n",
    "for questions in chinese_grouped.values():\n",
    "    for q in questions:\n",
    "        chinese_english.add(q['english'])\n",
    "\n",
    "if hindi_english == chinese_english:\n",
    "    print(\"âœ… SUCCESS: Both language sets have identical English questions\")\n",
    "    print(f\"   Total unique English questions: {len(hindi_english)}\")\n",
    "else:\n",
    "    print(\"âŒ ERROR: Question sets don't match!\")\n",
    "    print(f\"   Hindi has {len(hindi_english)} unique questions\")\n",
    "    print(f\"   Chinese has {len(chinese_english)} unique questions\")\n",
    "\n",
    "# Show sample comparison\n",
    "print(f\"\\nSample question comparison:\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Get first question from each language for comparison\n",
    "sample_hindi = hindi_questions[0] if hindi_questions else None\n",
    "sample_chinese = chinese_questions[0] if chinese_questions else None\n",
    "\n",
    "if sample_hindi and sample_chinese:\n",
    "    print(f\"English: {sample_hindi['english']}\")\n",
    "    print(f\"Hindi:   {sample_hindi['hindi']}\")\n",
    "    print(f\"Chinese: {sample_chinese['chinese']}\")\n",
    "    print(f\"Category: {sample_hindi['category']} | Complexity: {sample_hindi['complexity']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nâœ… Generated identical question sets for apples-to-apples comparison\")\n",
    "print(f\"   Total questions per language: {len(final_questions)}\")\n",
    "print(f\"   Languages: Hindi, Chinese\")\n",
    "print(f\"   Base: Same English questions translated to both languages\")\n",
    "\n",
    "print(f\"\\nBreakdown by category:\")\n",
    "all_categories = set()\n",
    "if hindi_grouped:\n",
    "    all_categories.update(hindi_grouped.keys())\n",
    "if chinese_grouped:\n",
    "    all_categories.update(chinese_grouped.keys())\n",
    "\n",
    "for category in sorted(all_categories):\n",
    "    hindi_count = len(hindi_grouped.get(category, []))\n",
    "    chinese_count = len(chinese_grouped.get(category, []))\n",
    "    print(f\"  {category}: {hindi_count} Hindi, {chinese_count} Chinese\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"READY FOR NOTEBOOK 3!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"1. Run notebook 3 to load these identical question sets\")\n",
    "print(f\"2. Compare Hindi vs Chinese performance on SAME questions\")\n",
    "print(f\"3. Ensure true apples-to-apples comparison across languages\")\n",
    "print(f\"4. Results will show language-specific differences, not question difficulty differences\")\n",
    "\n",
    "# Save a master comparison file for reference\n",
    "comparison_data = []\n",
    "for i, (h_q, c_q) in enumerate(zip(hindi_questions, chinese_questions)):\n",
    "    comparison_data.append({\n",
    "        'question_id': f'Q{i+1:03d}',\n",
    "        'english': h_q['english'],\n",
    "        'hindi': h_q['hindi'],\n",
    "        'chinese': c_q['chinese'],\n",
    "        'category': h_q['category'],\n",
    "        'complexity': h_q['complexity'],\n",
    "        'expected_source': h_q['expected_source']\n",
    "    })\n",
    "\n",
    "comparison_file = QUESTIONS_DIR / 'question_comparison.json'\n",
    "with open(comparison_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(comparison_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nðŸ“‹ Saved master comparison file: {comparison_file}\")\n",
    "print(f\"   This file shows all questions side-by-side for verification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f574869-e062-480c-a39f-202ca1221df6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MUL-RAG Research",
   "language": "python",
   "name": "mul-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
